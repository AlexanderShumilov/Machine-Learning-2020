{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "From the lecture notes (Lecture 11, slide 27), we know the expression for MAP for $w$:\n",
    "\n",
    "$$\n",
    "w_* = \\underset{w}{\\mathrm{argmin}}\\big[ \\cfrac{1}{2\\sigma^2}\\sum\\limits(w^Tx_i - y_i)^2 + \\cfrac{1}{2\\alpha}ww^T) \\big]\n",
    "$$\n",
    "\n",
    "If we divide the expression we want to minimize by $\\cfrac{1}{2\\sigma^2} > 0$, it will not affect $w_*$. Thus we will get $L_2$ regularization problem with parameter $\\lambda = \\cfrac{\\sigma^2}{\\alpha}$:\n",
    "\n",
    "$$\n",
    "w_* = \\underset{w}{\\mathrm{argmin}}\\big[\\sum\\limits(w^Tx_i - y_i)^2 + \\lambda\\|w\\|^2) \\big]\n",
    "$$\n",
    "\n",
    "We will solve it in matrix form (as we did in lecture 2, but without $b$ - coefficient matrix $X$ will consist just of $x_1, \\ldots, x_m$):\n",
    "\n",
    "$\n",
    "W_* = \\underset{W}{\\mathrm{argmin}} F(W), \\ F(W) = \\|XW - Y\\| + \\|W\\|^2 \\rightarrow  0 = \\nabla F(W) = 2\\lambda W+2X^T(XW - Y) \\rightarrow (X^TX+\\lambda I)W = (1 + \\lambda) W = X^TY \\Rightarrow W = \\cfrac{X^TY}{1+ \\lambda} = \\big(1 + \\cfrac{\\sigma^2}{\\alpha}\\big)^{-1}X^TY = \\big(\\cfrac{\\alpha}{\\sigma^2 + \\alpha}\\big)X^TY\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "\n",
    "First - what we know:\n",
    "\n",
    "$$\\sigma_{n}(x_{*}) = k(x_{*}, x_{*}) - k_{*}^{T}(K_n + \\sigma^{2} I_n)^{-1} k_{*}$$\n",
    "$$\\sigma_{n-1}(x_{*}) = k(x_{*}, x_{*}) - k_{*}^{T}(K_{n-1} + \\sigma^{2} I_{n-1})^{-1} k_{*}$$\n",
    "\n",
    "$\\sigma_{n} - \\sigma_{n-1} = k_{*}^{T}(K_{n-1}+\\sigma^{2} I_{n-1})^{-1} k_{*} - k_{*}^{T}(K_n + \\sigma^{2} I_n)^{-1} k_{*}$ - should be less than zero.\n",
    "\n",
    "Let's denote $\\tilde{K}_n = K_n + \\sigma^{2} I_n$. Then let's write $\\tilde{K}_n$ in block matrix form (straightforward from definition) - $x_1, \\ldots , x_{n-1}, x_n$ - given points:\n",
    "\n",
    "$$\n",
    "\\tilde{K}_n = \n",
    "\\begin{bmatrix}\n",
    "K_{n-1} + \\sigma^2 I_{n-1} & k_{n-1}(x_n)\\\\k_{n-1}(x_n)^{T} & k(x_n,x_n) + \\sigma^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    " \n",
    "\n",
    "\n",
    "We need somehow to process inverse of $\\tilde{K}_n$ in this expression. I had several ideas:\n",
    "- use Caushy Interlacing theorem to transform this problem to problem of consideration of eigenvalues of matrices of interest (in other words to prove that with our matrices with kernels inside we will not achive increase in eigenvalues with addition of one row and column to \"old\" matrix). I did not succeed :)\n",
    "- Get an expression of inverted matrix in form that will help us to estimate the sign of difference between given variances. In course of NLA we had task in homework related to the so-called Schur-complement of a block matrix (which will be rather convenient in this case):\n",
    "\n",
    "Let $A = \\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix}$ be a block matrix. The goal in that particular homework was to solve the linear system\n",
    "$$\n",
    "\\begin{bmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} f_1 \\\\ f_2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Using block elimination find matrix $S$ and right-hand side $\\hat{f_2}$ so that $u_2$ can be found from $S u_2 = \\hat{f_2}$ (matrix $S$ is called Schur complement </font> of the block $A_{11}$). The estimation follows:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "A_{11} & A_{12} \\\\ \n",
    "A_{21} & A_{22} \n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix} \n",
    "L_{11} & 0 \\\\ \n",
    "L_{21} & L_{22} \n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix} \n",
    "U_{11} & U_{12} \\\\ \n",
    "0 & U_{22} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "L_{11}U_{11} & L_{11}U_{12} \\\\ \n",
    "L_{21}U_{11} & L_{21}U_{12} + L_{22}U_{22} \n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\begin{cases}\n",
    "  A_{11} = L_{11}U_{11} \\Rightarrow U_{11} = L_{11}^{-1}A_{11} \\\\\n",
    "  A_{12} = L_{11}U_{12} \\Rightarrow U_{12} = L_{11}^{-1}A_{12} \\\\\n",
    "\\end{cases} \\Rightarrow\n",
    "\\begin{cases}\n",
    "  A_{21} = L_{21}U_{11} \\Rightarrow L_{21} = A_{21}U_{11}^{-1} = A_{21}A_{11}^{-1}L_{11} \\\\\n",
    "  L_{22}U_{22}  = A_{22} - L_{21}U_{12} = A_{22} - A_{21}{A_{11}}^{-1}L_{11}L_{11}^{-1}A_{12} = \n",
    "  A_{22} - A_{21}A_{11}^{-1}A_{12}\\\\\n",
    "\\end{cases} \\\\ \n",
    "\\text{Let } S = L_{22}U_{22} = A_{22} - A_{21}A_{11}^{-1}A_{12} \\rightarrow\n",
    "\\hat{f_2} = Su_2 = (A_{22} - A_{21}A_{11}^{-1}A_{12})u_2 = \n",
    "A_{22}u_2 - A_{21}A_{11}^{-1}A_{12}u_2 = \n",
    "\\bigg\\\\ \n",
    "\\begin{bmatrix} \n",
    "A_{11} & A_{12} \\\\ \n",
    "A_{21} & A_{22} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "u_1 \\\\ \n",
    "u_2 \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "f_1 \\\\ \n",
    "f_2 \n",
    "\\end{bmatrix}\n",
    "\\bigg\\\\ =\n",
    "f_2 - A_{21}u_1 - A_{21}A_{11}^{-1}(f_1 - A_{11}u_1) =\n",
    "f_1 - A_{11}u_1 - A_{21}A_{11}^{-1}f_2 - A_{21}A_{11}^{-1}A_{11}u_1 = \n",
    "f_2 - A_{21}A_{11}^{-1}f_1 \\Rightarrow \\\\\n",
    "\\text{Answer:  }\n",
    "\\begin{cases}\n",
    "  S = A_{22} - A_{21}A_{11}^{-1}A_{12} \\\\\n",
    "  \\hat{f_2} = f_2 - A_{21}A_{11}^{-1}f_1\n",
    "\\end{cases} \\\\\n",
    "$\n",
    "\n",
    "We can see, that Schur complement has the structure of interest - invertion of some matrix multiplied by other matrices on both sides. It was just motivation (partly copypasted from my NLA/HW2 solutions) - then we will obtain exact form for our matrix. Let's express blocks of inverted block matrix in terms of blocks of original matrix. With result for $S$, derived above it will be much simpler to derive:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix} \n",
    "A_{11} & A_{12} \\\\ \n",
    "A_{21} & A_{22} \n",
    "\\end{bmatrix} \\rightarrow\n",
    "\\qquad A^{-1} = \n",
    "\\begin{bmatrix} \n",
    "\\hat A_{11} & \\hat A_{12} \\\\ \n",
    "\\hat A_{21} & \\hat A_{22} \n",
    "\\end{bmatrix} \\Rightarrow\n",
    "\\begin{cases}\n",
    "    \\hat A_{11}  = A_{11}^{-1} + A_{11}^{-1}A_{12}SA_{21}A_{11}^{-1}\\\\\n",
    "    \\hat A_{12}  = -A_{11}^{-1}A_{12} S\\\\\n",
    "    \\hat A_{21}  = -S  A_{21}A_{11}^{-1}\\\\\n",
    "    \\hat A_{22}  = S\\\\\n",
    "    S  =  (A_{22} - A_{21}A_{11}^{-1}A_{12})^{-1}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Writing out the decomposed inverse, we get \n",
    "$\n",
    "\\tilde{K}_n = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{K}_{n-1}^{-1} + \\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S k_{n-1}(x_{n})^{T}\\tilde{K}_{n-1}^{-1} & -\\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S \\\\ \n",
    "-S k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1}^{-1} & S \n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "In this case we will have \n",
    "$\n",
    "S  = (k(x_{n}, x_{n}) + \\sigma^2 - k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}))^{-1}, \\\\\n",
    "$\n",
    "\n",
    "On the next step we will compute $k_*^{T}\\tilde{K}_{n}^{-1}k_*$:\n",
    "$\n",
    "\\tilde{K}_n = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{K}_{n-1}^{-1} + \\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S k_{n-1}(x_{n})^{T}\\tilde{K}_{n-1}^{-1} & -\\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n})S \\\\ \n",
    "-S k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1}^{-1} & S \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{K}_{n-1}^{-1} + \\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S k_{n-1}(x_{n})^{T}\\tilde{K}_{n-1}^{-1} & -\\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S \\\\ \n",
    "-S k_{n-1}(x_{n})^{T}\\tilde{K}_{n-1}^{-1}  & S \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "k_{n-1}(x_*)\\\\\n",
    "k(x_{n}, x_*)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} k_{n-1}^{T}(x_*)\\tilde{K}_{n-1}^{-1} + k_{n-1}^{T}(x_*)\\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S k_{n-1}(x_{n})^{T}\\tilde{K}_{n-1}^{-1} - k(x_{n}, x_*) S k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1}^{-1} \\\\ \n",
    "-k_{n-1}^{T}(x_*)\\tilde{K}_{n-1}^{-1} k_{n-1}(x_{n}) S + k(x_{n}, x_*) S \n",
    "\\end{bmatrix} ^{T}\n",
    "\\begin{bmatrix}\n",
    "k_{n-1}(x_*)\\\\\n",
    "k(x_{n}, x_*)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Let's multiply obtained \"vectors\":\n",
    "$\n",
    "k_*^{T}(K_n + \\sigma^2I_n)^{-1}k_* = k_{n-1}^{T}(x_*) \\tilde{K}_{n-1} k_{n-1}(x_*) + k_{n-1}^{T}(x_*) \\tilde{K}_{n-1} k_{n-1}(x_{n})Sk_{n-1}(x_{n})^{T} \\tilde{K}_{n-1} k_{n-1}(x_*) - k(x_n, x_*) S k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1} k_{n-1}(x_*) - k_{n-1}(x_*)^{T} \\tilde{K}_{n-1} k_{n-1}(x_{n}) S k(x_n, x_*) + k(x_n, x_*) S k(x_n, x_*) =\n",
    "\\sigma_{n-1} + k_{n-1}^{T}(x_*) \\tilde{K}_{n-1} k_{n-1}(x_{n})Sk_{n-1}(x_{n})^{T} \\tilde{K}_{n-1} k_{n-1}(x_*) - k(x_n, x_*) S k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1} k_{n-1}(x_*) - k_{n-1}(x_*)^{T} \\tilde{K}_{n-1} k_{n-1}(x_{n}) S k(x_n, x_*) + k(x_n, x_*) S k(x_n, x_*) \\Rightarrow \\\\\n",
    "\\sigma_{n} - \\sigma_{n-1} = k_{n-1}^{T}(x_*) \\tilde{K}_{n-1} k_{n-1}(x_{n})Sk_{n-1}(x_{n})^{T} \\tilde{K}_{n-1} k_{n-1}(x_*) - k(x_n, x_*) S k_{n-1}(x_{n})^{T} \\tilde{K}_{n-1} k_{n-1}(x_*) - k_{n-1}(x_*)^{T} \\tilde{K}_{n-1} k_{n-1}(x_{n}) S k(x_n, x_*) + k(x_n, x_*) S k(x_n, x_*)\n",
    "$\n",
    "\n",
    "Here we need to estimate the sign of this expression. Real number $S$ is obviously (I suppose) is larger than zero, because this variable contain sum of 2 squares of variances (we can see it from definition of $\\sigma_{n-1}$) - so we can take corresponding terms into account (we will loose it). Let's denote $\\chi = k_{n-1}^{T}(x_*)\\tilde{K}_{n-1} k_{n-1}(x_{n})$ and rewrite (write initial variances and plugin basically everything that we achieved) this complicated expression as polynom of second degree: $P = - 2k(x_n, x_*)\\chi + \\chi^2 + k(x_n, x_*)$. Thus we have $k_*^{T}(K_n + \\sigma^2I_n)^{-1}k_* = k_{n-1}^{T}(x_*)\\tilde{K}_{n-1} k_{n-1}(x_*) + (\\chi - k(x_n, x_*))^2 = k_{n-1}(x_*)^{T}(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x_*) + \\tfrac{1}{S}(\\chi - k(x_n, x_*))^2 \\geq k_{n-1}(x_*)^{T}(K_{n-1} + \\sigma^2I_{n-1})^{-1}k_{n-1}(x_*)$\n",
    "\n",
    "So we finally proved this inequality.\n",
    "\n",
    "Useful link - http://www.cari-info.org/cari-2012/session%203/3A2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you have gaussian distribution on $R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$.Get an expression for the correlation between the implementation of a Gaussian process  $y(x) ∼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution\n",
    "\n",
    "$y(x) ∼ GP (0, k(x, x ^{\\prime})) \\Rightarrow \\mu(y) = \\mathbb{E}y(x) = 0$\n",
    "\n",
    "By definition  $\\mathrm{Corr}(y, \\cfrac{\\partial y(\\tilde x)}{\\partial \\tilde x}) =\n",
    "\\cfrac{\\mathrm{Cov}(y, \\cfrac{\\partial y(\\tilde x)}{\\partial \\tilde x})}{\\sigma_{y}\\sigma_{\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}}}=\n",
    "\\cfrac{\\mathbb{E}\\bigg(\\big(y(x) - \\mathbb{E}y(x)\\big)\\big(\\cfrac{\\partial y(\\tilde x)}{\\partial \\tilde x} - \\mathbb{E}\\cfrac{\\partial y(\\tilde x)}{\\partial \\tilde x}\\big)\\bigg)}{\\sigma_{y}\\sigma_{\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}}}$.\n",
    "\n",
    "To derive correlation we will use following properties: $\\mathbb{E}(\\cfrac{\\partial y}{\\partial \\tilde x}) = \n",
    "\\cfrac{\\partial}{\\partial \\tilde x}\\mathbb{E}(y) = 0$, because expectation (first moment) is computed via converging (in terms of this task) integral, so we can interchange order of integration and derivation. Thus, via dragging one of multipliers into integral of expectation and rearanging the order like above we will get (far below we should note that $\\tilde x_1, \\tilde x_2$ are dummy variables!):\n",
    "\n",
    "$\\mathrm{Cov}(y, \\cfrac{\\partial y(\\tilde x)}{\\partial \\tilde x}) = E(y(x) \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x})) = E(\\cfrac{\\partial[y(x)y(\\tilde x)]}{\\partial \\tilde x}) = \\cfrac{\\partial}{\\partial \\tilde x}E(y(x)y(\\tilde x)) = \\cfrac{\\partial k(x, \\tilde x)}{\\partial \\tilde x}$\n",
    "\n",
    "$\\sigma_{y}^2 = \\mathbb{E}y(x)y(\\tilde x) -  (\\mathbb{E}y)^2 = \\mathbb{E}y^2 = \\mathbb{E}y(x)y(\\tilde x) = k(x, \\tilde x)$\n",
    "\n",
    "$\\sigma_{\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}}^2 = \n",
    "\\mathbb{E}\\big(\\frac{\\partial y(\\tilde x_1)}{\\tilde x_1} \\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2}\\big) -  (\\mathbb{E}\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x})^2 = \n",
    "\\mathbb{E}\\big(\\frac{\\partial y(\\tilde x_1)}{\\partial \\tilde x_1} \\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2}\\big) =\n",
    "\\bigg/ \\frac{\\partial}{\\partial \\tilde x_1} \\big(\\frac{\\partial}{\\partial \\tilde x_2}(y(\\tilde x_1)y(\\tilde x_2))\\big) = \n",
    "\\frac{\\partial}{\\partial \\tilde x_1} \\big(y(\\tilde x_2)\\frac{\\partial y(\\tilde x_1)}{\\partial \\tilde x_2} + y(\\tilde x_1) \\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2}\\big) = \n",
    "\\frac{\\partial}{\\partial \\tilde x_1} \\big(y(\\tilde x_1)\\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2}\\big) = \n",
    "\\frac{\\partial y(\\tilde x_1)}{\\partial \\tilde x_1} \\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2} + \n",
    "y(\\tilde x_1) \\frac{\\partial}{\\partial \\tilde x_1}(\\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2}) = \\frac{\\partial y(\\tilde x_1)}{\\partial \\tilde x_1} \\frac{\\partial y(\\tilde x_2)}{\\partial \\tilde x_2}\n",
    "\\bigg/ = \\mathbb{E} \\frac{\\partial}{\\partial \\tilde x_1} \\big(\\frac{\\partial}{\\partial \\tilde x_2}(y(\\tilde x_1)y(\\tilde x_2))\\big) = \\frac{\\partial}{\\partial \\tilde x_1} \\frac{\\partial}{\\partial \\tilde x_2}\\big(\\mathbb{E} y(\\tilde x_1)y(\\tilde x_2)\\big) = \\frac{\\partial}{\\partial x} \\frac{\\partial}{\\partial \\tilde x}\\big(\\mathbb{E} y(x)y(\\tilde x)\\big) = \\cfrac{\\partial^2 k(x, \\tilde x)}{\\partial x \\partial \\tilde x}$ . \n",
    "\n",
    "Overall:\n",
    "\n",
    "$\\mathrm{Corr}(y, \\cfrac{\\partial y(\\tilde x)}{\\partial \\tilde x}) = \\cfrac{\\frac{\\partial k(x, \\tilde x)}{\\partial \\tilde x}}{\\sqrt{k(x, \\tilde x) \\cdot \\frac{\\partial^2 k(x, \\tilde x)}{\\partial x \\partial \\tilde x}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the distance between 2 feature-mapping functions let's consider square of distance between $\\phi(x), \\phi(x')$:\n",
    "\n",
    "$\\| \\phi(x) - \\phi(x') \\|^2 = \\| \\phi(x) \\|^2 - 2\\langle\\phi(x), \\phi(x') \\rangle + \\| \\phi(x') \\|^2 = \\langle\\phi(x), \\phi(x)\\rangle - 2\\langle\\phi(x), \\phi(x') \\rangle + \\langle\\phi(x'), \\phi(x')\\rangle = K(x,x) -2K(x,x') + K(x',x')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the target vector $y$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**:\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$|$L$|$h_0$|$f_1$|$$y-f_1$$|$L$|$h_1$|$f_2$|$$y - f_2$$|$L$|$h_2$|$F_3$|\n",
    "|------|------|-----|-----------|---|-----|-----|---------|---|-----|-----|-----------|---|-----|-----|\n",
    "|  10  |  1   |  12.40  | -11.40  | 53.44 |  -7.40  |  -7.40  | 8.40 | 16.93 |  10.99 |  3.52  |    -2.55   | 8.90 |  -2.58  |  0.97  | \n",
    "|  32  |  9   |  12.40  | -3.40 | 20.95 |   -7.40 |  -7.40  | 16.40 | 12.93 |  10.99  |  3.59  |    5.41    | 7.23 |  0.65  |  4.24  |\n",
    "|  46  |  13  |  12.40  | 0.60 | 16.93 |  4.93  |  4.93  | 8.07 | 16.93 |  10.99  |  15.92  |    -2.92  | 7.57 |  0.65  |  16.57  |\n",
    "|  54  |  16  |  12.40  | 3.60 | 19.83 |  4.93  |  4.93  | 11.07 | 13.80 |  10.99  |  15.92  |    0.08   | 8.90 |  0.65  |  16.57  |\n",
    "|  63  |  23  |  12.40  | 10.60 | 25.35 | 4.93  |  4.93   | 18.07 | 8.90 |  18.07  |  23.00  |    0.00   | 8.90 |  0.65  |  23.65 |\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_0 =  [12.4 12.4 12.4 12.4 12.4]\n",
      "y - f_0 =  [-11.4  -3.4   0.6   3.6  10.6]\n"
     ]
    }
   ],
   "source": [
    "#\"Manual\" solution:\n",
    "y = np.array([1, 9, 13, 16, 23])\n",
    "\n",
    "print('f_0 = ', np.mean(y)*np.ones(len(y)))\n",
    "print('y - f_0 = ', y - np.mean(y)*np.ones(len(y)))\n",
    "# To compute Loss I used the formula above (I assume that by manual course instructors mean that we can just solve \n",
    "# problem on paper and we do not have to attach any photos of simple derivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L =  [53.44 20.95 16.93 19.83 25.35]\n",
      "f_1 =  [-7.4  -7.4   4.93  4.93  4.93]\n",
      "y - f_1 =  [ 8.4  16.4   8.07 11.07 18.07]\n"
     ]
    }
   ],
   "source": [
    "L = np.array([53.44, 20.95, 16.93, 19.83, 25.35])\n",
    "f_1 = np.array([-7.40, -7.40, 4.93, 4.93, 4.93])\n",
    "\n",
    "print('L = ', L)\n",
    "print('f_1 = ', f_1)\n",
    "print('y - f_1 = ', y - f_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L =  [16.93 12.93 16.93 13.8   8.9 ]\n",
      "h_1 =  [10.95 10.99 10.99 10.99 18.07]\n",
      "f_2 =  [ 3.55  3.59 15.92 15.92 23.  ]\n",
      "y - f_2 =  [-2.55  5.41 -2.92  0.08  0.  ]\n"
     ]
    }
   ],
   "source": [
    "L = np.array([16.93, 12.93, 16.93, 13.80, 8.90])\n",
    "h_1 = np.array([10.95, 10.99, 10.99, 10.99, 18.07])\n",
    "f_2 = h_0 + h_1\n",
    "\n",
    "print('L = ', L)\n",
    "print('h_1 = ', h_1)\n",
    "print('f_2 = ', f_2)\n",
    "print('y - f_2 = ', y - f_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L =  [8.9  7.23 7.57 8.9  8.9 ]\n",
      "h_2 =  [-2.58  0.65  0.65  0.65  0.65]\n",
      "F_3 =  [ 0.97  4.24 16.57 16.57 23.65]\n"
     ]
    }
   ],
   "source": [
    "L = np.array([8.90, 7.23, 7.57, 8.90, 8.90])\n",
    "h_2 = np.array([-2.58, 0.65, 0.65, 0.65, 0.65])\n",
    "F_3 = h_0 + h_1 + h_2\n",
    "print('L = ', L)\n",
    "print('h_2 = ', h_2)\n",
    "print('F_3 = ', F_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return F_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10,32,46,54,63]\n",
    "y = [1, 9, 13, 16, 23]\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "F3 = F_3\n",
    "splits = [46, 54, 32]\n",
    "\n",
    "boosted_F_plot = plot_tree(x, F3, stumps = list(np.sort(splits)))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "ax.plot(x_range,boosted_F_plot,'--', linewidth=2, label = 'composite function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From lectures we know, that $\\sum\\limits_{t=1}^{T} \\alpha_t h_t$ is sum of interest in this problem. \n",
    "\n",
    "1. $N_t = \\frac{1}{2} \\Rightarrow \\alpha = \\log\\bigg(\\cfrac{1 - N_t}{N_t}\\bigg) = \\log(1) = 0$. It means that corresponding weak hypothesis from sum above will not contribute (so we just ignore it)\n",
    "\n",
    "2. $N_t > \\frac{1}{2} \\Rightarrow \\alpha = \\log\\bigg(\\cfrac{1 - N_t}{N_t}\\bigg) < 0$. It means that corresponding classifier will have weight $<0$ in sum, so the predictition of this classifier will be \"other way around\", as far as I understand. If classifier predicts something with negative weight, sign of multiplication containing this alpha will be the same as for this positive alpha and negative $h_t$, which leads to the inverse prediction.\n",
    "\n",
    "3. $N_t = 0 \\Rightarrow \\alpha = \\log\\bigg(\\cfrac{1 - N_t}{N_t}\\bigg) \\rightarrow \\infty$ (in reality it means that $\\alpha$ will be big). It means that corresponding prediction of corresponding classifier will have very big weight and so it will be considered as most valuable classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
