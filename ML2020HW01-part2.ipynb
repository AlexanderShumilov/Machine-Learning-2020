{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:\n",
    "\n",
    "We basically should write regression problem for both cases and estimate criteria for matrix $A$ (since we have to find certain matrix $A$) to fulfill these conditions.\n",
    "\n",
    "Lasso regression problem for feature matrix $Z = XA \\quad $: $||Zw - y||_2^2 + \\lambda||w||_{1} =  ||XAw - y||_2^2 + \\lambda||w||_{1} \\rightarrow \\min_{\\boldsymbol{w}}$.  Let $Aw = \\hat{w} \\Rightarrow w = A^{-1}\\hat{w} \\Rightarrow$ Lasso optimization problem will look like $||X\\hat{w} - y||_2^2 + \\lambda||A^{-1}\\hat{w}||_{1} \\rightarrow \\min_{\\boldsymbol{\\hat{w}}}$.\n",
    "\n",
    "In comparison with initial linear problem for $\\hat{w}$ : $||X\\hat{w} - y||_2^2 + \\lambda||\\hat{w}||_{\\infty}\\rightarrow \\min_{\\boldsymbol{\\hat{w}}}$. So  basically we need to find such A that $||A^{-1}\\hat{w}||_{1} = ||w||_{1} = \\sum_{i=1}^{n}|w_i| = \\max_{i}|(Aw)_i| = ||\\hat{w}||_{\\infty}$.\n",
    "\n",
    "We can choose invertible matrix $A$ that, for example, after multiplication by vector $w$ will have some coordinate exactly equal to sum of modulus of coordinates of $w$:  \n",
    "$$\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "sign(w_1) & sign(w_2)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Thus $Aw = (w_1 \\quad w_1sign(w_1)+w_2sign(w_2))^T = (w_1 \\quad|w_1|+|w_2|)^T$. Obviously absolute value of second coordinate is bigger, so we will indeed get an equality above.\n",
    "\n",
    "($\\det A \\neq 0$ - obvious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:\n",
    "To write down correct optimization problem we should use Log-likelihood notation (minus log-likelihood function). Let the size of training sample $S$ be N:\n",
    "$L(w) = -\\cfrac{1}{N}\\sum\\limits_{i=1}^{N}log(\\Phi(y_ix_i^Tw))$\n",
    "\n",
    "\n",
    "Next we will follow standard procedure (also discussed in canvas): for $x \\in \\mathbb{R}^d, y = \\{+1; -1\\}$ our function $\\Phi(x)$ is strictly monotone (as CDF for $N(0,1)$). Due to the fact that this strictly monotone sigmoidal function will have certain properties: $\\Phi(-\\infty)=0, \\quad \\Phi(+\\infty)=1, \\quad \\Phi(-x) = 1 - \\Phi(x)$.\n",
    "\n",
    "Let's denote $\\hat{w}$ as the hyperplane that will separate our classes: $\\forall i \\quad y_{i}\\langle x_{i},w^{*}\\rangle>0$ for all $i$. We can consider slightly modified the separating hyperplane $\\hat{w}s$, where s is parameter. Thus $\\lim_{s\\rightarrow +\\infty}L(\\hat{w}s) = -\\cfrac{1}{N}\\sum\\limits_{i=1}^{N}log(\\Phi(y_ix_i^T \\hat{w}s)) = 0$ due to exponential decay of function for large $s$.\n",
    "\n",
    "Next we can use our properties for $\\Phi$: $\\Phi(y_ix_i^T \\hat{w}s)\\bigg|_{s \\rightarrow +\\infty} \\rightarrow 1 \\Rightarrow$ choosing bigger and bigger $s$ makes Log-likelihood tend to zero, but never reach it exactly like $\\Phi$ itself. This is exactly means that initial problem do not have optimum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem was discussed during last seminar, so there is basically nothing to change... The pipeline of the task is kind of straightforward - use basic properties from probability theory and separate 2 logarithms in the end.\n",
    "\n",
    "Bayesian decision rule:\n",
    "$$\\hat{y} = \\underset{y \\in \\{C_1\\dots C_k\\}}{\\operatorname{argmax}} p(y) \\prod_{i=1}^n p(x_i|y)=\\big[\\textbf{apply} \\log\\big]=\\underset{y \\in \\{C_1\\dots C_k\\}}{\\operatorname{argmax}} \\big[\\log p(y) + \\sum_{i=1}^n \\log p(x_i|y)\\big]=$$\n",
    "$$= \\underset{y \\in \\{C_1\\dots C_k\\}}{\\operatorname{argmax}} \\big[\\log p(y) + \\sum_{i=1}^n \\big(x_i \\cdot \\log p(x_i=1|y) + (1 - x_i) \\cdot \\log p(x_i=0|y) \\big)\\big]=$$\n",
    "$$= \\underset{y \\in \\{C_1\\dots C_k\\}}{\\operatorname{argmax}} \\big[\\log p(y) + \\sum_{i=1}^n \\big(x_i \\cdot \\log \\frac{p(x_i=1|y)}{p(x_i=0|y)} + \\log p(x_i=0|y) \\big)\\big]=$$\n",
    "$$= \\underset{y \\in \\{C_1\\dots C_k\\}}{\\operatorname{argmax}} \\big[\\underbrace{\\big(\\log p(y)+\\sum_{i=1}^{n}\\log p(x_i=0|y)\\big)}_{b^{y}} + \n",
    "\\underbrace{\\sum_{i=1}^n x_i\\cdot\\log \\frac{p(x_i=1|y)}{p(x_i=0|y)}}_{w^{y}x}\\big]$$\n",
    "Thus, Bayesian Naive Classifier can be exactly presented like $\\operatorname{argmax}$ of linear functions of the input $\\Rightarrow$ we have a linear decision rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\cdot\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg),$$\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "### Your solution:\n",
    "In this task we should explain connection between two formulas and provide some mathematical proof.  Let's take a closer look to the $L_kOCV$ process. We have $m \\in [1; k]$, where k is the number of samples that are used for testing in each iteration. Remaining $n - m$ elements are left for training. So accroding to the given classification problem we should pick $n-m-1$ elements from $n-k-1$ elements, so the number of all possible combinations is $C_{n-k-1}^{n-m-1}$ (we also do not want to pick element with same label) - thus we basically calculated the number of subsets of $X^{n}$ with dimension cardinality equal $k$ and can change quite abstract outer summation by summation by closest neighbours. The number of combination of choosing k samples from data sample is $C_{n}^{k}$. Next we can drag combination coefficient under the sum and change it:\n",
    "\n",
    "$$\\cfrac{C_{n-1}^{k-1}}{k} = \\cfrac{(n-1)!}{k(n-k)!(k-1)!} = \n",
    "\\cfrac{(n)!}{n(n-k)!(k)!} = \\cfrac{C_{n}^{k}}{n} \\Rightarrow \n",
    "\\cfrac{1}{n}\\cfrac{1}{C_{n-1}^{k-1}} = \\cfrac{1}{k}\\cfrac{1}{C_{n}^{k}} \\Rightarrow$$ Finally we can rearange the multipliers in initial sum and get the answer: \n",
    "$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\cdot\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k} \\frac{1}{k}\\bigg(\\sum\\limits_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg) = \n",
    "\\sum\\limits_{i=1}^{n}\\sum_{m=1}^{k}\\cfrac{1}{\n",
    "kC_{n}^{k}}[y_{i}\\neq y_{i}^{m}]C_{n-1-m}^{n-k-1} =\n",
    "\\sum\\limits_{i=1}^{n}\\sum\\limits_{m=1}^{k}\\cfrac{1}{\n",
    "nC_{n-1}^{k-1}}[y_{i}\\neq y_{i}^{m}]C_{n-1-m}^{n-k-1} =\n",
    "\\sum\\limits_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:\n",
    "During 5th seminar we discussed exact problem. Via bootstrap procedure we randomly select one of $n$ points from $X^n$ and make such selection $n$ times to get new sample of $n$ points $\\hat{X}^n$. The probabiity to select one point from $X^n$ is $\\frac{1}{n}$, so the probability of $\\textbf{not}$ to select a point is $1 - \\frac{1}{n}$. We continue the selection procedure $n$ times $\\Rightarrow$ full probability is $\\big( 1 - \\frac{1}{n}\\big)^n$ - basic fact from probability theory. Using another basic fact,from mathematical analysis this time, we can estimate the limit:\n",
    "$$ \\lim_{n \\rightarrow \\infty} \\big( 1 - \\frac{1}{n}\\big)^n = \n",
    " \\lim_{n \\rightarrow \\infty} \\big( 1 + \\frac{(-1)}{n}\\big)^n =\n",
    " e^{-1} \\approx 0.37$$\n",
    " So after bootstrap procedure we ignored about a third of all points from initial sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n",
    "\n",
    "\n",
    "\n",
    "### Your solution:\n",
    "Similar problem was also discussed on previous seminar. Here we should find\n",
    "$\\hat{y}\\in\\underset{\\hat{y}\\in\\mathbb{R}}{\\operatorname{argmin}}\\frac{1}{n}\\sum\\limits_{i=1}^{n}L(y_i,\\hat{y})$ to minimize the error.\n",
    "\n",
    "\n",
    "1. $L(y,\\hat{y})=[y\\neq \\hat{y}] =\n",
    "\\begin{cases}\n",
    "1, y \\neq \\hat{y}\\\\\n",
    "0, y = \\hat{y}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Thus $\\cfrac{1}{n}\\sum_{i=1}^{n}L(y_i,\\hat{y}) = \\cfrac{1}{n}(n-1)$, because one term of the sum will always be 0 and others always be 1. So $\\hat{y} = y_i$ - answer.\n",
    "\n",
    "2. Here we have to differentiate our target function with respect to $\\hat{y}$:\n",
    "\n",
    "$0 = \\cfrac{\\partial}{\\partial \\hat{y}}\\cfrac{1}{n}\\sum\\limits_{i=1}^{n}\\frac{(y_i-\\hat{y})^{2}}{y_i^2}=\\cfrac{2}{n}\\sum_{i=1}^{n}\\cfrac{(y_i-\\hat{y})}{y_i^2} = \\sum_{i=1}^{n}\\cfrac{1}{y_i} - \\sum_{i=1}^{n}\\cfrac{\\hat{y}}{y_i^2} \\Rightarrow \\sum_{i=1}^{n}\\cfrac{1}{y_i} = \\sum_{i=1}^{n}\\cfrac{\\hat{y}}{y_i^2}$\n",
    "$\\Rightarrow \\hat{y} = \\cfrac{\\sum\\limits_{i=1}^{n}\\cfrac{1}{y_i}}{\\sum\\limits_{i=1}^{n}\\cfrac{1}{{y_i}^2}}$\n",
    "\n",
    "We also can transform all fractions and get $\\hat{y} = \\cfrac{\\sum\\limits_{i=1}^{n}y_i \\prod\\limits_{i=1}^{n}y_i}{\\sum\\limits_{i=1}^{n}y_i^2}$, but it will not become prettier :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$.\n",
    "### Your solution:\n",
    "\n",
    "Let's consider we have certain curve on ROC-diagram. The diagram is basically is a grid with ordinate coordinates corresponding to $1$ among correct labels of the test object, abcissa - $0$. Thus we have certain amounts of blocks on the grid corresponding to the all possible pairs (object of class $1$, object of class $2$). If our algorithm predicted order correctly (1 scored more than 0), then corresponding block will be including into out ROC AUC We can move along the curve from node to node, so in this terms the area under curve is the ratio of correctly ordered pairs of our objects. Now we want to get it in mathematical expression (hello again, probability theory).\n",
    "\n",
    "Let's consider $y_i$ - class of the i-object, $k$ - number of object in the test, $a_i$ - algorithm answer for the i-object. $I$ - indicator function. So we can write expression for this ratio via: \n",
    "$\n",
    "\\begin{equation}\n",
    "    \\begin{matrix}\n",
    "    I[y_i < y_j] & =\n",
    "    & \\left\\{\n",
    "    \\begin{matrix}\n",
    "    0 & \\mbox{if } y_i\\geq y_j \\\\\n",
    "    1 & \\mbox{if } y_i<y_j\n",
    "    \\end{matrix} \\right.\n",
    "    \\end{matrix}\n",
    "\\end{equation}\n",
    "$, \n",
    "$\n",
    "\\begin{equation}\n",
    "    \\begin{matrix}\n",
    "    \\hat{I} [a_i < a_j] & =\n",
    "    & \\left\\{\n",
    "    \\begin{matrix}\n",
    "    0 & \\mbox{if } a_i>a_j \\\\\n",
    "    0.5 & \\mbox{if } a_i=a_j \\\\\n",
    "    1 & \\mbox{if } a_i<a_j\n",
    "    \\end{matrix} \\right.\n",
    "    \\end{matrix}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Since we have ($f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$), one of the functions will slightly transform (we do not have intermediate situations, only $0$ or $1$ - so, again, $a_i$ - what we get from algorithm, $y_i$ - class to compare):\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{matrix}\n",
    "    \\hat{I} [a_i < a_j] & =\n",
    "    & \\left\\{\n",
    "    \\begin{matrix}\n",
    "    0 & \\mbox{if } a_i>a_j \\\\\n",
    "    1 & \\mbox{if } a_i<a_j\n",
    "    \\end{matrix} \\right.\n",
    "    \\end{matrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Thus we can find how many pairs were classified correctly (the order is same): $\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{k}I[y_i < y_j]\\hat{I}[a_i < a_j] = \\sum\\limits_{i<j}[y_{i}<y_{j}]$. Overall we have $\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{k}I[y_i < y_j]$ pairs. Number of pairs from the other side is simply  $n_{1}n-n_{1}n_{1} =  n_{1}(n-n_{1})$ (again, looking on $n_{1}$).\n",
    "\n",
    "Thus, $S_{\\text{ROC}} = \\cfrac{\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{k}I[y_i < y_j]\\hat{I}[a_i < a_j]}{\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{k}I[y_i < y_j]} = \n",
    "\\cfrac{\\sum\\limits_{i<j}[y_{i}<y_{j}]}{\\sum\\limits_{i=1}^{k}\\sum\\limits_{j=1}^{k}I[y_i < y_j]} = \n",
    "\\cfrac{1}{n_{1}(n-n_{1})}\\sum\\limits_{i<j}[y_{i}<y_{j}]$\n",
    "\n",
    "Sources (can be downloaded via sci-hub.si): \n",
    "\n",
    "- https://link.springer.com/article/10.1023/A:1010920819831\n",
    "- http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.458.8392&rep=rep1&type=pdf\n",
    "- https://dyakonov.org/2017/07/28/auc-roc-%D0%BF%D0%BB%D0%BE%D1%89%D0%B0%D0%B4%D1%8C-%D0%BF%D0%BE%D0%B4-%D0%BA%D1%80%D0%B8%D0%B2%D0%BE%D0%B9-%D0%BE%D1%88%D0%B8%D0%B1%D0%BE%D0%BA/\n",
    "\n",
    "First 2 sources contain great explanation what is actually ROC AUC in terms of analysis and probabilistic theory, such as connection (rescaling actually) with so-called Mann–Whitney U-statistic, third russian sourse basicaly is the same, but gives more practical formula for derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:\n",
    "1) \n",
    "$K(x,y)=\\langle x, y \\rangle ^ d$:\n",
    "Let's consider $x, y \\in \\mathcal{R}^n: \\langle x, y\\rangle^d = \\sum\\limits_{i=1}^n (x_i y_i)^d = \\sum\\limits_{|\\alpha| = d} {n \\choose \\alpha} (x y)^\\alpha$, where $\\alpha = (\\alpha_1 \\ldots \\alpha_n)$ - multi-index: $|\\alpha| = \\alpha_1 + \\ldots + \\alpha_n$, and $(x y)^\\alpha = (x_1 y_1)^{\\alpha_1}\\ldots(x_n y_n)^{\\alpha_n}$. ${n \\choose \\alpha}$ then is multin0mial coefficient (basicaly whole expression is expansion of Newton binom formula on a higher dimensional case). \n",
    "So here we can clearly see, how we can distinguish components of vectors $x$ and $y$: \n",
    "$\\sum\\limits_{|\\alpha| = d}^n {n \\choose \\alpha} (x y)^\\alpha = \\sum\\limits_{\\alpha_1 + \\ldots + \\alpha_n = d}^n {n \\choose \\alpha} (x_1 y_1)^{\\alpha_1}\\ldots(x_n y_n)^{\\alpha_n}$. We have different collections of possible $\\alpha$ representations: $(d, 0, \\ldots, 0), (d-1, 1, 0, \\ldots, 0) \\ldots, (0, \\ldots, 0, d)$, so overall our sum will look like this: $x_1^d y_1^d + M_{d-1}^d x_1^{d-1}x_2 y_1^{d-1} y_2 + \\ldots + x_2^d y_2^d + \\ldots \\Rightarrow$, so we can organize our function $\\varphi$ in this way: $\\varphi(z) = (z_1^d, z_2^d, \\ldots, z_n^d, \\sqrt{M_{d-1}^{d}}z_1^{d-1}z_2^1, \\ldots)$, where $M$ is corresponding multinomial coefficient.  \n",
    "\n",
    "For the case of arbitrary $d$ the task seems to be a bit not representative, so let's confirm it for $d = 2$, $n = 2$ (because for larger $n$ it will be rather obvious, at least we can use mathematical induction for larger dimensions): \n",
    "$\\varphi(z) = (z_1^2, z_2^2, \\sqrt{2}z_1z_2) \\Rightarrow \\langle \\varphi(x), \\varphi(y) \\rangle ^ 2 = (x_1y_1)^2 + 2(x_1y_1x_2y_2) + (x_2y_2)^2 = \\sum\\limits_{i=1}^2 (x_i y_i)^2 = \\langle x, y\\rangle^2$\n",
    "\n",
    "\n",
    "2) \n",
    "This is inhomogeneous problem number 1, so the solution will be slightly different - most part of our vector function will remain the same, but several new terms in sum will appear. Let's look on the example for $d=2, n=2$:\n",
    "\n",
    "$\\left(c + \\langle x, y \\rangle \\right)^ 2 = c^2 + 2c\\langle x, y \\rangle + \\langle x, y \\rangle ^2 = c^2 + 2cx_1y_1 + 2cx_2y_2 + (x_1y_1 + x_2y_2)^2\\Rightarrow \\varphi(z)_{d = 2} = (c, \\sqrt{2c} z_1, \\sqrt{2c} z_2, z_1^2, z_2^2, \\sqrt2 z_1 z_2)$\n",
    "\n",
    "Thus we can improve the first formula: $\\varphi(z) = (\\sqrt{c^d}, \\sqrt{M_{d-1}^{d}c^{d-1}}z_1, \\sqrt{M_{d-1}^{d}c^{d-1}}z_2, \\ldots,        \\sqrt{M_{d-2}^{d}c^{d-2}}z_2^2, \\ldots, \\sqrt{M_{d-1}^{d}M_{d-2}^{d-1}c^{d-1}}z_1^{d-1}z_2,\\ldots, z_1^d, z_2^d, \\ldots, z_n^d, \\sqrt{M_{d-1}^{d}}z_1^{d-1}z_2^1, \\ldots)$ \n",
    "\n",
    "To the old part remains the same, but also we will have new terms with respect to the parameter $c$ (taking into account changing of its power) - looks very complicated, but with the simple example above it should be clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n",
    "### Your solution:\n",
    "First let us note that $e^{-\\|x-x'\\|^{2}} = e^{-\\|x\\|^{2}}e^{2x^Tx'}e^{-\\|x'\\|^{2}}$ according to simple linear algebra and mathematical analysis.\n",
    "\n",
    "In our proof we will need definition and some properties of positive kernels:\n",
    "\n",
    "$\\textbf{Definition}$: Kernel $K(x_1, x_2) : X * X \\rightarrow \\mathbb{R} $ is positive definite, if it is symmetric with respect to $x_1, x_2$ and $\\sum\\limits_{i,j}^n c_i c_j K(x_i, x_j) \\geq 0$ for any $c_i, c_j \\in \\mathbb{R}$ and for all finite sequences of points $(x_1,\\ldots,x_n)$.\n",
    "\n",
    "If $K_1, K_2$ positive definite kernels (here and later I denote it as PDK) then:\n",
    "* $\\alpha K_1 + \\beta K_2$ - PDK\n",
    "* $K_1K_2$ - PDK\n",
    "\n",
    "First property is rather obvious (in definition of positive definite kernel we can use that sum is linear operation straightforward), second is not so obvious, but managable (if two Hermitian matrices $K_1$ and $K_2$ are\n",
    "positive semidefinite, so is their component-wise product - there is corresponding lemma in linear algebra as well)\n",
    "\n",
    "As we can see,  if we can prove that all three exponents are PDK, then we will solve this task:\n",
    "\n",
    " 1) Let's use Taylor expansion for middle exponent function: $e^{2x^Tx'} = \\sum\\limits_{k = 0}^n\\cfrac{(2x^Tx')^k}{k!}$. We know, that linear kenel is PDK, so from the first and second property we can conclude that $e^{2x^Tx'}$ is PDK.\n",
    " 2) Let $K'(x, x') = e^{-\\|x\\|^{2}}e^{-\\|x'\\|^{2}}\\Rightarrow$  \n",
    "$$K'(x, x') = f(x)f(x') = f(x)\\overline{f(x')} \\Rightarrow \\sum\\limits_{i,j}^n c_i c_j K'(x_i, x_j) = \\big|\\sum\\limits_{i}^n c_i f(x_i)\\big|^2 \\geq 0$$Thus $K'(x, x')$ is positive definite function. To prove that this is actually a kernel we can use Merser theorem for positive semi-definite functions (declares the relation between such functions and kernels - I suppose we don't need to prove this theorem here).\n",
    "\n",
    "So we again have product of two PDK and according to the second property, $K(x, x') = e^{-\\|x-x'\\|^{2}} = e^{2x^Tx'}e^{-\\|x\\|^{2}}e^{-\\|x'\\|^{2}}$ is indeen positive definite kernel. \n",
    "  \n",
    "Sources: For Merser theorem - https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf, \n",
    "https://en.wikipedia.org/wiki/Mercer%27s_theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq f_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
